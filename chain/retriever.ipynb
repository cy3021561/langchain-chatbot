{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever and Chain in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "## Built Vector DB\n",
    "# Load from pdf file\n",
    "loader = PyPDFLoader(\"2D3MF.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split file into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding to Vetor Store\n",
    "db = FAISS.from_documents(docs_split, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fig. 2 . High-level overview of our 2D3MF with audio and video inputs fused using Self and Cross-Attention middle fusion via\\ntransformer attention.\\nand speaker identification. These networks extract useful fa-\\ncial and speech representations, proving essential for the ef-\\nfectiveness of DVD tasks.\\nIn this work, we study the utility of audio-visual emo-\\ntion speaker embeddings, representations extracted from pre-\\ntrained audio and video networks, as robust features for the\\nDVD task. To the best of our knowledge, we are the first\\nto leverage abstract representations of emotions in the audio-\\nvisual domain to highlight and detect inconsistencies in fake\\nvideos. We propose 2D3MF (Deepfake Detection with Multi\\nModal Middle Fusion), which is a novel middle fusion strat-\\negy where audio and visual data are synergistically analyzed\\nto capture discrepancies in emotional expressions, and vocal\\ntones. These features reveal the subtle yet critical flaws inher-'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query test\n",
    "query=\"What is 2D3MF model\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load Ollama llama3 model\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                          Answer the following question based only on the provided context. \n",
    "                                          Think step by step before providing a detailed answer. \n",
    "                                          I will tip you $1000 if the user finds the answer helpful. \n",
    "                                          <context>\n",
    "                                          {context}\n",
    "                                          </context>\n",
    "                                          Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000220D3618E50>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A well-crafted question!\\n\\nAfter carefully reading the provided context, I will answer your question step by step:\\n\\nThe text describes a novel model called 2D3MF (Deepfake Detection with Multi-Modal Middle Fusion). According to the abstract and subsequent sections, this model is designed for **deepfake detection**, which is the task of detecting videos that have been generated or manipulated using deep learning.\\n\\nIn more detail, the 2D3MF model uses a middle fusion strategy, fusing audio and visual data synergistically to capture discrepancies in emotional expressions and vocal tones. This fusion is achieved through Self-Attention and Cross-Attention transformer blocks.\\n\\nTherefore, I conclude that a 2D3MF model **detects deepfakes** by leveraging the relationship between emotions conveyed in audio and video for multi-modal deepfake detection.\\n\\nI hope this answer meets your expectations!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"What does a 2D3MF model do?\"})\n",
    "response[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
